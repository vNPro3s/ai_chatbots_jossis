<!DOCTYPE html><html><head>
      <title>bigramToNN_model</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///C:\Users\Korisnik\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\katex\katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="from-bigram-to-simple-nn-language-model">From Bigram to simple NN language model</h1>

<div style="text-align:justify">
<hr>
<p><strong>author:</strong> Ante Panjkota, PhD</p>
<p><strong>date:</strong> december, 2022.</p>
<p><strong>version:</strong> ver 1.0</p>
<p><strong>credits go to:</strong>  Andrej Karpathy (<a href="https://www.youtube.com/watch?v=PaCmpygFfXo&amp;t=5400s">Intro to Language Modeling</a>)</p>
<p><strong>status:</strong> in progress</p>
<hr>
<p>In the lecture on the probabilistic model, we introduced a formal definition of that model. The fundamental premise is that we can predict the next word based on the previous one. To show how that model works, we need a more straightforward example. In that sense, we will simplify our problem and apply the same logic from the probabilistic model to individual words, more precisely to personal names. Personal names are listed in <code>hr_names.txt</code> file so we can import them:</p>
<pre data-role="codeBlock" data-info="Python" class="language-python"><span class="token keyword keyword-with">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&quot;names_hr.txt&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">&quot;utf-8&quot;</span><span class="token punctuation">)</span> <span class="token keyword keyword-as">as</span> tfile<span class="token punctuation">:</span>
    <span class="token builtin">all</span> <span class="token operator">=</span> tfile<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
names <span class="token operator">=</span> <span class="token builtin">all</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;,&quot;</span><span class="token punctuation">)</span>
names <span class="token operator">=</span> <span class="token punctuation">[</span>name<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">&quot; ,\n&quot;</span><span class="token punctuation">)</span> <span class="token keyword keyword-for">for</span> name <span class="token keyword keyword-in">in</span> names<span class="token punctuation">]</span>  
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;total names in list: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>names<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</pre><p>If we want to see only the first ten names from a list and, after that, count how many names in total start with some letter (e.g., with &quot;s&quot;), we can run the following code:</p>
<pre data-role="codeBlock" data-info="Python" class="language-python"><span class="token comment"># first ten from list</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;First ten from list of names:&quot;</span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>names<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># starts wit some letter</span>
<span class="token keyword keyword-def">def</span> <span class="token function">starts_with</span><span class="token punctuation">(</span>ltr<span class="token operator">=</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    starts_with <span class="token operator">=</span> <span class="token punctuation">[</span>name <span class="token keyword keyword-for">for</span> name <span class="token keyword keyword-in">in</span> names <span class="token keyword keyword-if">if</span> name<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span>ltr<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>starts_with<span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;total: &quot;</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>starts_with<span class="token punctuation">)</span><span class="token punctuation">)</span>
ltr <span class="token operator">=</span> <span class="token string">&quot;p&quot;</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;Starts with letter: &quot;</span><span class="token punctuation">,</span> ltr<span class="token punctuation">)</span>
starts_with<span class="token punctuation">(</span>ltr<span class="token punctuation">)</span>
</pre><p>Output is:</p>
<pre data-role="codeBlock" data-info class="language-"><code>First ten from list of names:
[&apos;Velimir&apos;, &apos;&#x17D;elimir&apos;, &apos;Vitomir&apos;, &apos;Vilim&apos;, &apos;Tomislav&apos;, &apos;Toma&apos;, &apos;Kre&#x161;imir&apos;, &apos;Branimir&apos;, &apos;&#x106;iril&apos;, &apos;Trpimir&apos;]
Starts with letter:  p
[]
total:  0
</code></pre><p>Output is not as we expected it to be - what can be a reason for a such outcome? The problem is related to the mix of upper and lower cases in names. Therefore, it is suitable to perform the simplest normalization in the form of <code>Case Folding</code>.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># Case Folding</span>
names <span class="token operator">=</span> <span class="token punctuation">[</span>name<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-for">for</span> name <span class="token keyword keyword-in">in</span> names<span class="token punctuation">]</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>names<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;-----------------------------------------------------------------------&quot;</span><span class="token punctuation">)</span>
sltr <span class="token operator">=</span> <span class="token string">&quot;p&quot;</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;Starts with letter: &quot;</span><span class="token punctuation">,</span> sltr<span class="token punctuation">)</span>
starts_with<span class="token punctuation">(</span>sltr<span class="token punctuation">)</span>
</pre><p>Output:</p>
<pre data-role="codeBlock" data-info class="language-"><code>[&apos;velimir&apos;, &apos;&#x17E;elimir&apos;, &apos;vitomir&apos;, &apos;vilim&apos;, &apos;tomislav&apos;, &apos;toma&apos;, &apos;kre&#x161;imir&apos;, &apos;branimir&apos;, &apos;&#x107;iril&apos;, &apos;trpimir&apos;]
------------------------------------------------------------------------------
Starts with letter:  p
[&apos;perunko&apos;, &apos;pepo&apos;, &apos;pavel&apos;, &apos;plamen&apos;, &apos;plamenko&apos;, &apos;petar&apos;, &apos;patrik&apos;, &apos;predrag&apos;, &apos;petra&apos;, &apos;perica&apos;, &apos;petrunjela&apos;, &apos;pavle&apos;]
total:  12
</code></pre><p>Using the same principle, we can get the same counts for all starting letters, but our goal is different. We are interested in the <strong>bigram version</strong> of the known probabilistic model for predicting the following letter knowing the previous one. With that knowledge, such a bigram model can generate new personal name. Luckily, we can calculate bigrams probabilities from all names. But first, we must form all bigrams from names and calculate their occurrences. We will employ the &quot;unique character&quot; <code>.</code> to denote the name&apos;s start and end.</p>
<pre data-role="codeBlock" data-info="Python" class="language-python"><span class="token comment"># creating bigrams</span>
coded_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;.&quot;</span> <span class="token operator">+</span> name <span class="token operator">+</span> <span class="token string">&quot;.&quot;</span> <span class="token keyword keyword-for">for</span> name <span class="token keyword keyword-in">in</span> names<span class="token punctuation">]</span>
bigrams <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token keyword keyword-for">for</span> cname <span class="token keyword keyword-in">in</span> coded_names<span class="token punctuation">:</span>
    letters <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>cname<span class="token punctuation">)</span>
    <span class="token keyword keyword-for">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword keyword-in">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>letters<span class="token punctuation">,</span> letters<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        bigram <span class="token operator">=</span> ch1 <span class="token operator">+</span> <span class="token string">&quot;,&quot;</span> <span class="token operator">+</span> ch2
        bigrams<span class="token punctuation">[</span>bigram<span class="token punctuation">]</span> <span class="token operator">=</span> bigrams<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bigram<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>bigrams<span class="token punctuation">)</span>
</pre><p>This code will produce and print all bigrams from input names and corresponding occurrences in key-value pairs. We can extract all letters represented in the input &quot;corpus&quot; of names from keys.</p>
<pre data-role="codeBlock" data-info="Python" class="language-python">keys <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>bigrams<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
letters <span class="token operator">=</span> <span class="token string">&quot; &quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>keys<span class="token punctuation">)</span>
letters <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span> <span class="token comment"># get only unique letters</span>
letters <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span> <span class="token comment"># transforming again in the list data structure</span>
</pre><p>This code produces letters, white space, punctuation marks <code>.</code> and <code>,</code>. To get rid of  uninformative chars we can simply use <code>remove</code> method from the list object.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Number of letters before removal: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">\n</span><span class="token interpolation"><span class="token punctuation">{</span>letters<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
letters<span class="token punctuation">.</span>remove<span class="token punctuation">(</span><span class="token string">&quot;,&quot;</span><span class="token punctuation">)</span>
letters<span class="token punctuation">.</span>remove<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Number of letters after removal procedure: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">\n</span><span class="token interpolation"><span class="token punctuation">{</span>letters<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</pre><p>Output:</p>
<pre data-role="codeBlock" data-info class="language-"><code>Number of letters before removal: 30
[&apos;&#x17E;&apos;, &apos;m&apos;, &apos;g&apos;, &apos;a&apos;, &apos;.&apos;, &apos;,&apos;, &apos;k&apos;, &apos;v&apos;, &apos;&#x10D;&apos;, &apos;l&apos;, &apos;e&apos;, &apos;o&apos;, &apos;d&apos;, &apos;i&apos;, &apos;f&apos;, &apos;j&apos;, &apos;&#x161;&apos;, &apos;h&apos;, &apos;&#x107;&apos;, &apos;&#x111;&apos;, &apos;u&apos;, &apos;b&apos;, &apos;p&apos;, &apos; &apos;, &apos;z&apos;, &apos;n&apos;, &apos;r&apos;, &apos;t&apos;, &apos;s&apos;, &apos;c&apos;]
Number of letters after removal procedure: 28
[&apos;&#x17E;&apos;, &apos;m&apos;, &apos;g&apos;, &apos;a&apos;, &apos;.&apos;, &apos;k&apos;, &apos;v&apos;, &apos;&#x10D;&apos;, &apos;l&apos;, &apos;e&apos;, &apos;o&apos;, &apos;d&apos;, &apos;i&apos;, &apos;f&apos;, &apos;j&apos;, &apos;&#x161;&apos;, &apos;h&apos;, &apos;&#x107;&apos;, &apos;&#x111;&apos;, &apos;u&apos;, &apos;b&apos;, &apos;p&apos;, &apos;z&apos;, &apos;n&apos;, &apos;r&apos;, &apos;t&apos;, &apos;s&apos;, &apos;c&apos;]
</code></pre><p>Putting all letters from the CRO alphabet in one set and these letters from input names in another set make it easy to define missing letters with no name candidate that starts with it in the input list.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">hr_ltrs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;b&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&#x10D;&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&#x107;&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;d&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;d&#x17E;&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&#x111;&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;e&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;f&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;h&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;i&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;j&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;k&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;l&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;lj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;m&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;n&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;nj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;o&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;p&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;s&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&#x161;&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;t&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;u&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;v&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;z&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&#x17E;&quot;</span><span class="token punctuation">]</span>
<span class="token comment"># transforming to sets</span>
ltrs <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span>
hrs <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>hr_ltrs<span class="token punctuation">)</span>
not_starting_letters <span class="token operator">=</span> hrs<span class="token punctuation">.</span>difference<span class="token punctuation">(</span>ltrs<span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>not_starting_letters<span class="token punctuation">)</span>
</pre><p>Getting all letters that are in our model and not in hr_letters is trivial.  Nevertheless, it is helpful to define auxiliary dictionaries with letters and numbers for our modeling purpose.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">letters<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># gives sorted letters</span>
ltoi <span class="token operator">=</span> <span class="token punctuation">{</span>l<span class="token punctuation">:</span>k <span class="token keyword keyword-for">for</span> k<span class="token punctuation">,</span>l <span class="token keyword keyword-in">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">}</span>
itol <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span>l <span class="token keyword keyword-for">for</span> k<span class="token punctuation">,</span> l <span class="token keyword keyword-in">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>ltoi<span class="token punctuation">,</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>itol<span class="token punctuation">)</span>
</pre><p>Which gives:</p>
<pre data-role="codeBlock" data-info class="language-"><code>{&apos;.&apos;: 0, &apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3, &apos;d&apos;: 4, &apos;e&apos;: 5, &apos;f&apos;: 6, &apos;g&apos;: 7, &apos;h&apos;: 8, &apos;i&apos;: 9, &apos;j&apos;: 10, &apos;k&apos;: 11, &apos;l&apos;: 12, &apos;m&apos;: 13, &apos;n&apos;: 14, &apos;o&apos;: 15, &apos;p&apos;: 16, &apos;r&apos;: 17, &apos;s&apos;: 18, &apos;t&apos;: 19, &apos;u&apos;: 20, &apos;v&apos;: 21, &apos;z&apos;: 22, &apos;&#x107;&apos;: 23, &apos;&#x10D;&apos;: 24, &apos;&#x111;&apos;: 25, &apos;&#x161;&apos;: 26, &apos;&#x17E;&apos;: 27}

{0: &apos;.&apos;, 1: &apos;a&apos;, 2: &apos;b&apos;, 3: &apos;c&apos;, 4: &apos;d&apos;, 5: &apos;e&apos;, 6: &apos;f&apos;, 7: &apos;g&apos;, 8: &apos;h&apos;, 9: &apos;i&apos;, 10: &apos;j&apos;, 11: &apos;k&apos;, 12: &apos;l&apos;, 13: &apos;m&apos;, 14: &apos;n&apos;, 15: &apos;o&apos;, 16: &apos;p&apos;, 17: &apos;r&apos;, 18: &apos;s&apos;, 19: &apos;t&apos;, 20: &apos;u&apos;, 21: &apos;v&apos;, 22: &apos;z&apos;, 23: &apos;&#x107;&apos;, 24: &apos;&#x10D;&apos;, 25: &apos;&#x111;&apos;, 26: &apos;&#x161;&apos;, 27: &apos;&#x17E;&apos;}
</code></pre><p>Now, all prerequisites are present to build bigram model that can be visualized. For that sake tensor representation will be used.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword keyword-import">import</span> torch
N <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>letters<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
N<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
</pre><p>This is an empty 28x28 tensor with all zero elements. Tensor size corresponds to the all possible combination of bigrams from input names.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword keyword-for">for</span> k <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">.</span>size<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword keyword-for">for</span> l <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">.</span>size<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        big <span class="token operator">=</span> itol<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">&quot;,&quot;</span> <span class="token operator">+</span> itol<span class="token punctuation">[</span>l<span class="token punctuation">]</span>
        N<span class="token punctuation">[</span>k<span class="token punctuation">,</span>l<span class="token punctuation">]</span> <span class="token operator">=</span> bigrams<span class="token punctuation">.</span>get<span class="token punctuation">(</span>big<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
N<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
</pre><p>Visualization:</p>
<pre data-role="codeBlock" data-info="Python" class="language-python"><span class="token keyword keyword-import">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword keyword-as">as</span> plt

plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>N<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">&quot;Greens&quot;</span><span class="token punctuation">)</span>
<span class="token keyword keyword-for">for</span> k <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">.</span>size<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword keyword-for">for</span> l <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">.</span>size<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>text<span class="token punctuation">(</span>k<span class="token punctuation">,</span> l<span class="token punctuation">,</span> N<span class="token punctuation">[</span>l<span class="token punctuation">,</span>k<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ha<span class="token operator">=</span><span class="token string">&quot;center&quot;</span><span class="token punctuation">,</span> va<span class="token operator">=</span><span class="token string">&quot;top&quot;</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&quot;grey&quot;</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>text<span class="token punctuation">(</span>k<span class="token punctuation">,</span>l<span class="token punctuation">,</span> itol<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token operator">+</span>itol<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> ha<span class="token operator">=</span> <span class="token string">&quot;center&quot;</span><span class="token punctuation">,</span> va<span class="token operator">=</span><span class="token string">&quot;bottom&quot;</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&quot;grey&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token string">&quot;off&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</pre><center>
<p><img src="https://dub01pap002files.storage.live.com/y4m986foh03JKxvQm8yemGbeYQNbcQXVURgE4Gubkz_ymV1XKcLW6TvlTq0lp3dzCFW5-bfMV5W3L9AQPkCN-xwzWTtWK6G0VXTJmQUglHkCjHNFEivfbX8BB1x8moAzrf0SGk55yf2kxHDMqQLgqCEPn5LMTxSDsY4E2pgoglU_zWWi7Kj_70D_mpuiGF9c8PB?width=1600&amp;height=1600&amp;cropmode=none" alt="imgBigrams"></p>
<p><strong>Figure 1</strong> Bigram model visualization</p>
</center>
<p>The main question is, &quot;How to obtain new names generated with this model trained on input data???&quot; From statistics - a sample can be generated from some known distribution.<br>
Let us observe simplified example &#x2192; [A, B, C, D, E] is set of five elements with probabilities [0.35, 0.2, 0.1, 0.2, 0.15]. We wants to generate 15 samples from that distribution.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">p <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.35</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.15</span><span class="token punctuation">]</span>
p <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
sample <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>p<span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> replacement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
sample
</pre><p>Output:</p>
<pre data-role="codeBlock" data-info class="language-"><code>tensor([3, 0, 2, 0, 0, 3, 0, 4, 4, 1, 1, 0, 0, 1, 2])
</code></pre><p>In this sample, the most frequent element is A, and the least frequent is B, as expected because A has the largest probability, and B is the lowest.  Using similar reasoning new names can be generated from built bigram model. In that sense we need to get probabilities instead counts. To get that every row will be divided by corresponding sum:</p>
<pre data-role="codeBlock" data-info="Python" class="language-python">rw_sums <span class="token operator">=</span> N<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
probs <span class="token operator">=</span> N <span class="token operator">/</span> rw_sums
probs<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># gives 1.0</span>
</pre><pre data-role="codeBlock" data-info="python" class="language-python">gen <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span>
<span class="token keyword keyword-for">for</span> _ <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    ix <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-while">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> replacement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>gen<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-if">if</span> ix <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            res <span class="token operator">=</span> <span class="token string">&quot;&quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>name<span class="token punctuation">)</span>
            <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>
            <span class="token keyword keyword-break">break</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            name<span class="token punctuation">.</span>append<span class="token punctuation">(</span>itol<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">)</span>
</pre><p>This code will produce ten new names proposed by the simple bigram model. Here, we are interested in the performances of this model. For the sake of a simplified problem, we are interested in the probabilities of a combination of some letters that form a particular name.</p>
<pre data-role="codeBlock" data-info="Python" class="language-python">log_likelihood <span class="token operator">=</span> <span class="token number">0.0</span>
n <span class="token operator">=</span> <span class="token number">0</span>
name <span class="token operator">=</span> <span class="token string">&quot;&#x17E;arko&quot;</span>
<span class="token keyword keyword-for">for</span> lt <span class="token keyword keyword-in">in</span> <span class="token punctuation">[</span>name<span class="token punctuation">]</span><span class="token punctuation">:</span>
    chrs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;.&quot;</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>lt<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">&quot;.&quot;</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-for">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword keyword-in">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>chrs<span class="token punctuation">,</span> chrs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        idx1 <span class="token operator">=</span> ltoi<span class="token punctuation">[</span>ch1<span class="token punctuation">]</span>
        idx2 <span class="token operator">=</span> ltoi<span class="token punctuation">[</span>ch2<span class="token punctuation">]</span>
        prob_big <span class="token operator">=</span> probs<span class="token punctuation">[</span>idx1<span class="token punctuation">,</span> idx2<span class="token punctuation">]</span>
        log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>prob_big<span class="token punctuation">)</span>
        log_likelihood <span class="token operator">+=</span> log_probs
        n<span class="token operator">+=</span><span class="token number">1</span>
        <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span>ch1<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>ch2<span class="token punctuation">}</span></span><span class="token string">: prob: </span><span class="token interpolation"><span class="token punctuation">{</span>prob_big<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> | logprob: </span><span class="token interpolation"><span class="token punctuation">{</span>log_probs<span class="token punctuation">:</span><span class="token format-spec"> .4f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;log_likelihood: </span><span class="token interpolation"><span class="token punctuation">{</span>log_likelihood<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;loss as neg log likelihood: &quot;</span><span class="token punctuation">,</span> <span class="token operator">-</span>log_likelihood<span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;avg neg log likelihood: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token operator">-</span>log_likelihood<span class="token operator">/</span>n<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</pre><p>The test name is a familiar name for our model, so we have a pretty good outcome. But, if we put some odd name for our mode, e.g., <code>jack</code> the final result will be:</p>
<pre data-role="codeBlock" data-info class="language-"><code>. j: prob: 0.0434 | logprob: -3.1384
j a: prob: 0.4487 | logprob: -0.8014
a c: prob: 0.0000 | logprob: -inf
c k: prob: 0.0000 | logprob: -inf
k .: prob: 0.0897 | logprob: -2.4108
log_likelihood: -inf
loss as neg log likelihood:  tensor(inf)
avg neg log likelihood: inf
</code></pre><p>In this case, the average negative log-likelihood loss function tells us how good or bad our model performs on individual names. As the outcome, we intend to have minimal loss as possible. However, in this case, the loss is infinite, so the model&apos;s performance on the target name is poor. The reason is apparent and straightforward &#x21A0; in our model, there is no combination in which <code>a</code> is followed by <code>c</code>, and <code>c</code> is never followed by <code>k</code>. To escape such cases, we can perform so-called <strong>smoothing</strong> by adding one to all counts (CNT). As a result of that operation, when the model faces unusual combinations, it will assign minimal probabilities, producing significant loss but not infinite.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">CNT <span class="token operator">=</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>CNT<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
CNT
</pre><p>All counts are raised by one, so probabilities would change, and in the final corresponding average log-likelihood, a loss measure for this model. This change will produce following result for the <code>jack</code></p>
<pre data-role="codeBlock" data-info class="language-"><code>. j: prob: 0.0428 | logprob: -3.1517
j a: prob: 0.3396 | logprob: -1.0799
a c: prob: 0.0030 | logprob: -5.8201
c k: prob: 0.0263 | logprob: -3.6376
k .: prob: 0.0755 | logprob: -2.5840
log_likelihood: -16.27325439453125
loss as neg log likelihood:  tensor(16.2733)
avg neg log likelihood: 3.254650831222534
</code></pre><p>For all names in our dataset result would be:</p>
<pre data-role="codeBlock" data-info class="language-"><code>Output exceeds the size limit. Open the full output data in a text editor
. p: prob: 0.0348 | logprob: -3.3593
p r: prob: 0.0408 | logprob: -3.1987
r e: prob: 0.0583 | logprob: -2.8430
e d: prob: 0.0599 | logprob: -2.8154
d r: prob: 0.1215 | logprob: -2.1079
r a: prob: 0.1748 | logprob: -1.7444
a g: prob: 0.0208 | logprob: -3.8742
g .: prob: 0.0500 | logprob: -2.9957
...
o .: prob: 0.3403 | logprob: -1.0779

log_likelihood: -5400.92236328125
loss as neg log likelihood:  tensor(5400.9224)
avg neg log likelihood: 2.3189876079559326
</code></pre><p>Our probabilistic bigram model could be better, so we must consider other candidates. One such candidate is the so-called artificial neural network (ANN). Before that, some introduction related to the real neurons are welcomed.</p>
<center>
<p><img src="https://dub01pap002files.storage.live.com/y4mSc4mZXa5mgFVv63HEDoqGQG65RMmDfv_dthsMXiNqaIq16qYjmN0IJu_zs2qkBfjfvgTWbLurxH6sU43rFwY8ctnN0L5GsjOtUE0YPnivLTYOLDXH4Rfea-vc56lgMxKdIdAU9Wf8m0nrPZotTKdB0oYHXcT3N49Tc0ihHgu7k-Em6Fx2MTMAVk9paXM6jTZ?width=934&amp;height=507&amp;cropmode=none" alt="neuron"></p>
<p><strong>Figure 2</strong> Biological Neuron<br>
(Source: <a href="https://upload.wikimedia.org/wikipedia/commons/b/b5/Neuron.svg">https://upload.wikimedia.org/wikipedia/commons/b/b5/Neuron.svg</a> , accessed: December, 2022., license: CC)</p>
</center>
<p>Our best mathematical model for that neuron is depicted on figure 3:</p>
<center>
<p><img src="https://dub01pap002files.storage.live.com/y4mo8r-dIn9fRhVXcVOX1ziXPE4OlbP3JwQHM6svdyPKOnNMXSy13jr-EcY0B7YeFr4Q9SCnYHupqdAtWfPJgZsv-m-gjLGPoYtXwfWi5GQXfl24oGtsjeDz3P8gGKjfxL91bHrvStaLSXJqRiB-PtpnTf5qT0Az-EppTSLCE6UTx3qbuLQ19RWJEE1MHqinmY0?width=698&amp;height=520&amp;cropmode=none" alt="artificial_neuron"></p>
<p><strong>Figure 3</strong> Artificial neuron<br>
(Source: <a href="https://freesvg.org/artificial-neuron">https://freesvg.org/artificial-neuron</a>, accessed: December, 2022. license: CC)</p>
</center>
<p>For now, we will assume that simplified neuron only performs summation of multiplication of all inputs with so-called weights. Assuming only one name from our model, inputs are first letters of bigram pairs, and outputs are corresponding following letters.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">xs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
ys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword keyword-for">for</span> w <span class="token keyword keyword-in">in</span> no_duplicates<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    chrs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;.&quot;</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">&quot;.&quot;</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-for">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword keyword-in">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>chrs<span class="token punctuation">,</span> chrs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        big <span class="token operator">=</span> ch1 <span class="token operator">+</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> ch2
        <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>big<span class="token punctuation">)</span>
        xs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ltoi<span class="token punctuation">[</span>ch1<span class="token punctuation">]</span><span class="token punctuation">)</span>
        ys<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ltoi<span class="token punctuation">[</span>ch2<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># transforming into tensors</span>
xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>ys<span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;inputs: </span><span class="token interpolation"><span class="token punctuation">{</span>xs<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;outputs: </span><span class="token interpolation"><span class="token punctuation">{</span>ys<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</pre><p>Output is:</p>
<pre data-role="codeBlock" data-info class="language-"><code>. p
p r
r e
e d
d r
r a
a g
g .
inputs: tensor([0, 16, 17, 5, 4, 17, 1, 7])
outputs: tensor([16, 17, 5, 4, 17, 1, 7, 0])
</code></pre><p>Observing this example conclusion is straightforward &#x2192; after (0), we wonts to p (16) to have a high probability, and after p (16) high chance of occurrence for r (17) would be good, and so on. So numbers are more appropriate for computer representation of letters than letters themselves. So, generalizing from this example to all names from the dataset, we can conclude that artificial neurons must have many inputs as the number of all possible letters from our input alphabet &#x2192; 28.</p>
<blockquote>
<p>To be correct, our model must include all letters from the Croatian alphabet plus one unique character . (dot) . Nevertheless, this shorter model is enough to show the main concepts and ideas.</p>
</blockquote>
<p>As dealing with 28 inputs every character can not be represented only with index number, but with specially coded input value that corresponds to that index. Coding needs to adhere to all inputs, so natural coding scheme is to use vector of the length equal to the number of inputs (28). In PyTorch we already have function for such transformation &#x2192; <code>TORCH.NN.FUNCTIONAL.ONE_HOT</code>.</p>
<pre data-role="codeBlock" data-info="Python" class="language-python"><span class="token keyword keyword-import">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword keyword-as">as</span> F

xenc <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>all_alph_in<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
yenc <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>ys<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>all_alph_in<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># let us visualize xenc</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>xenc<span class="token punctuation">)</span><span class="token punctuation">,</span> xenc<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> xenc<span class="token punctuation">.</span>dtype
</pre><p>Figure 4 depicts encoded inputs for one name.</p>
<center>
<p><img src="https://dub01pap002files.storage.live.com/y4mDfrs_drFtkmynOKfABoe5CknFr7RvSIQr-ISEJs8Eo3kJc0aDcSGiCuqRyxZpke-Iuqpy9xpv4gk1pXcrjNc_hIcKpD_G_XqxSafJ2acneGfbLfhurHmwsgl9JZLZu1OpDj24EQEoLRNzK7iBJux50YGtroIL8Eo3s04_riOgLr4XVqx7zpS4L-7A96M0xJ2?width=640&amp;height=480&amp;cropmode=none" alt="encoded"></p>
<p><strong>Figure 4</strong> Encoded inputs for chosen name</p>
</center>
<p>After successful encoding we need to obtain weights in random manner. Moreover, we want to have as many neurons as possible letters from input dataset. One neuron is not enough for solving this, and not only this problem (see. Minsky critique on Perceptron). Figure three suggests multiplication of these values with coded inputs:</p>
<pre data-role="codeBlock" data-info="python" class="language-python">W <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
logits <span class="token operator">=</span> xenc @ W
logits<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> logits<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">]</span>
</pre><p>The last result from the above code provides information about the value assigned by the 15th neuron for the fifth letter from the input sequence. Here we have only the multiplication of weights with inputs and no addition of so-called bias terms. Especially we also omit the activation function, so the final result is only a simple neural network with one linear layer consisting of 28 neurons. There is a question of how we can interpret the output of those neurons. It can be shown that those outputs are some log counts, and obvious exponentiation will give numbers that can be interpreted as counts. Moreover, probabilities are easy to get from that information by normalizing counts by rows sums.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token number">1</span><span class="token punctuation">:</span> counts <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
<span class="token number">2</span><span class="token punctuation">:</span> probs <span class="token operator">=</span> counts <span class="token operator">/</span> counts<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token number">3</span><span class="token punctuation">:</span> probs<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> probs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
</pre><p>Lines 1, and 2 are known as softmax which provides way to obtain probabilities from any linear layer. In the end, our simple NN provides probabilities of the predicted next letter for known inputs. We can summarize all that in the following manner:</p>
<pre data-role="codeBlock" data-info="python" class="language-python">sz <span class="token operator">=</span> xs<span class="token punctuation">.</span>size<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
nlls <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>sz<span class="token punctuation">)</span>
<span class="token keyword keyword-for">for</span> k <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>sz<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> xs<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> ys<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;***************************************************************************************&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;bigram example </span><span class="token interpolation"><span class="token punctuation">{</span>k<span class="token punctuation">}</span></span><span class="token string"> -&gt; </span><span class="token interpolation"><span class="token punctuation">{</span>itol<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>itol<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;Input to NN: &quot;</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;probabilities for that input:\n&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span>probs<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;label - actual next charater is: &quot;</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    p <span class="token operator">=</span> probs<span class="token punctuation">[</span>k<span class="token punctuation">,</span> y<span class="token punctuation">]</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;probability assigned by NN to correct character: &quot;</span><span class="token punctuation">,</span> p<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    logp <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;log likelihood: &quot;</span><span class="token punctuation">,</span> logp<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    nll <span class="token operator">=</span> <span class="token operator">-</span>logp
    <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;negative log likelihood: &quot;</span><span class="token punctuation">,</span> nll<span class="token punctuation">)</span>
    nlls<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> nll
<span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">&quot;Average nll: &quot;</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>nlls<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p>Output:</p>
<pre data-role="codeBlock" data-info class="language-"><code>***************************************************************************************
bigram example 0 -&gt; . p
Input to NN:  0
probabilities for that input:

tensor([0.0429, 0.0234, 0.0481, 0.0446, 0.0195, 0.0434, 0.0488, 0.0224, 0.0298,
        0.0246, 0.0242, 0.0454, 0.0388, 0.0286, 0.0431, 0.0424, 0.0193, 0.0263,
        0.0507, 0.0462, 0.0280, 0.0404, 0.0436, 0.0282, 0.0262, 0.0460, 0.0293,
        0.0459])
label - actual next charater is:  16
probability assigned by NN to correct character:  0.019288545474410057
log likelihood:  -3.9482438564300537
negative log likelihood:  tensor(3.9482)
***************************************************************************************
bigram example 1 -&gt; p r
Input to NN:  16
probabilities for that input:

tensor([0.0366, 0.0287, 0.0242, 0.0425, 0.0204, 0.0205, 0.0437, 0.0326, 0.0508,
        0.0505, 0.0380, 0.0266, 0.0230, 0.0477, 0.0307, 0.0357, 0.0305, 0.0541,
        0.0529, 0.0218, 0.0512, 0.0439, 0.0402, 0.0377, 0.0229, 0.0293, 0.0213,
        0.0419])
label - actual next charater is:  17
probability assigned by NN to correct character:  0.05409238860011101
log likelihood:  -2.9170618057250977
negative log likelihood:  tensor(2.9171)
***************************************************************************************
bigram example 2 -&gt; r e
Input to NN:  17
probabilities for that input:

tensor([0.0350, 0.0347, 0.0558, 0.0240, 0.0288, 0.0419, 0.0218, 0.0357, 0.0528,
        0.0345, 0.0323, 0.0258, 0.0528, 0.0344, 0.0386, 0.0222, 0.0540, 0.0418,
        0.0327, 0.0312, 0.0440, 0.0278, 0.0404, 0.0299, 0.0213, 0.0513, 0.0305,
        0.0237])
label - actual next charater is:  5
probability assigned by NN to correct character:  0.04190102592110634
log likelihood:  -3.172445058822632
negative log likelihood:  tensor(3.1724)
***************************************************************************************
bigram example 3 -&gt; e d
Input to NN:  5
probabilities for that input:

tensor([0.0302, 0.0396, 0.0331, 0.0448, 0.0450, 0.0286, 0.0515, 0.0280, 0.0256,
        0.0219, 0.0521, 0.0390, 0.0409, 0.0201, 0.0328, 0.0492, 0.0495, 0.0399,
        0.0311, 0.0228, 0.0478, 0.0210, 0.0348, 0.0340, 0.0243, 0.0297, 0.0507,
        0.0321])
label - actual next charater is:  4
probability assigned by NN to correct character:  0.04496588930487633
log likelihood:  -3.101850986480713
negative log likelihood:  tensor(3.1019)
***************************************************************************************
bigram example 4 -&gt; d r
Input to NN:  4
probabilities for that input:

tensor([0.0306, 0.0335, 0.0366, 0.0272, 0.0238, 0.0237, 0.0234, 0.0548, 0.0486,
        0.0346, 0.0235, 0.0364, 0.0355, 0.0301, 0.0240, 0.0484, 0.0336, 0.0504,
        0.0494, 0.0241, 0.0331, 0.0540, 0.0422, 0.0531, 0.0310, 0.0264, 0.0398,
        0.0283])
label - actual next charater is:  17
probability assigned by NN to correct character:  0.05041271075606346
log likelihood:  -2.9875118732452393
negative log likelihood:  tensor(2.9875)
***************************************************************************************
bigram example 5 -&gt; r a
Input to NN:  17
probabilities for that input:

tensor([0.0350, 0.0347, 0.0558, 0.0240, 0.0288, 0.0419, 0.0218, 0.0357, 0.0528,
        0.0345, 0.0323, 0.0258, 0.0528, 0.0344, 0.0386, 0.0222, 0.0540, 0.0418,
        0.0327, 0.0312, 0.0440, 0.0278, 0.0404, 0.0299, 0.0213, 0.0513, 0.0305,
        0.0237])
label - actual next charater is:  1
probability assigned by NN to correct character:  0.034737128764390945
log likelihood:  -3.3599462509155273
negative log likelihood:  tensor(3.3599)
***************************************************************************************
bigram example 6 -&gt; a g
Input to NN:  1
probabilities for that input:

tensor([0.0340, 0.0416, 0.0424, 0.0246, 0.0326, 0.0286, 0.0375, 0.0394, 0.0357,
        0.0473, 0.0282, 0.0278, 0.0213, 0.0412, 0.0265, 0.0270, 0.0491, 0.0289,
        0.0327, 0.0302, 0.0287, 0.0332, 0.0486, 0.0395, 0.0332, 0.0446, 0.0406,
        0.0551])
label - actual next charater is:  7
probability assigned by NN to correct character:  0.03943735733628273
log likelihood:  -3.233041763305664
negative log likelihood:  tensor(3.2330)
***************************************************************************************
bigram example 7 -&gt; g .
Input to NN:  7
probabilities for that input:

tensor([0.0235, 0.0430, 0.0389, 0.0236, 0.0586, 0.0445, 0.0392, 0.0398, 0.0487,
        0.0434, 0.0275, 0.0391, 0.0297, 0.0304, 0.0341, 0.0330, 0.0320, 0.0393,
        0.0265, 0.0259, 0.0233, 0.0311, 0.0395, 0.0455, 0.0411, 0.0473, 0.0271,
        0.0244])
label - actual next charater is:  0
probability assigned by NN to correct character:  0.02346174046397209
log likelihood:  -3.7523841857910156
negative log likelihood:  tensor(3.7524)
-----------------------------------------------------------------------------------
Average nll:  3.30906081199646

</code></pre><p>We can luckily end up with a better outcome by changing initial random values for weights. Still, we want to end up with appropriate weights with an optimization method linked with ANN that minimizes loss, known as gradient descent.</p>
</div>
<hr>

  <link rel="stylesheet" type="text/css" media="all" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">


      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>